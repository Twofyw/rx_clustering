{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Spark and build Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T07:31:49.453274Z",
     "start_time": "2019-07-14T07:31:36.789100Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import findspark\n",
    "findspark.init('/home/ywx-data/spark/spark-2.4.3-bin-hadoop2.7')\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T07:31:50.145939Z",
     "start_time": "2019-07-14T07:31:49.457128Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(None).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T07:31:50.166379Z",
     "start_time": "2019-07-14T07:31:50.148696Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://gpuserver2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f612067dba8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and show raw data with Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T07:32:00.812120Z",
     "start_time": "2019-07-14T07:31:56.510446Z"
    }
   },
   "outputs": [],
   "source": [
    "df_raw = spark.read.csv(\"lh_tp_node_ui.csv\", header=True, inferSchema=True)\n",
    "input_lh_tp_node_ui = df_raw.filter('lv == 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T07:32:43.616678Z",
     "start_time": "2019-07-14T07:32:43.609896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LV: integer (nullable = true)\n",
      " |-- NODE: long (nullable = true)\n",
      " |-- L1: string (nullable = true)\n",
      " |-- DATA_TIME: timestamp (nullable = true)\n",
      " |-- U: double (nullable = true)\n",
      " |-- IR: double (nullable = true)\n",
      " |-- IX: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T07:32:42.091130Z",
     "start_time": "2019-07-14T07:32:41.858643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+---+-------------------+-----------+------------+------------+\n",
      "| LV|        NODE| L1|          DATA_TIME|          U|          IR|          IX|\n",
      "+---+------------+---+-------------------+-----------+------------+------------+\n",
      "|  1|354010389381|  A|2019-04-18 08:00:00|229.1714286| 7.873935201| 20.07064388|\n",
      "|  1|354010389388|  A|2019-04-20 20:00:00|    225.875| 8.138034736| 6.975577399|\n",
      "|  1|354010389381|  A|2019-04-18 08:30:00|229.9428571| -0.98901879|-9.586840019|\n",
      "|  1|354010389386|  A|2019-04-19 12:30:00|     228.24|-0.100734942| 0.968079481|\n",
      "|  1|354019022841|  A|2019-04-20 13:15:00|      228.0| 1.269984777| 4.580591726|\n",
      "|  1|354013332709|  A|2019-04-18 01:30:00|231.7666667|  12.4127701|-2.413129017|\n",
      "|  1|354010389386|  A|2019-04-18 13:45:00|     229.26|-7.112555611| 12.62412582|\n",
      "|  1|354010389388|  A|2019-04-20 09:45:00|     227.55| 16.40039602| 1.763531565|\n",
      "|  1|354010389383|  A|2019-04-20 13:45:00|228.5428571| 19.45274478|-8.603976663|\n",
      "|  1|354013332709|  A|2019-04-20 14:00:00|229.1166667| 3.353540984| 7.726937515|\n",
      "|  1|354019022839|  A|2019-04-20 18:15:00|228.0857143| 3.748100646|-4.459997783|\n",
      "|  1|354010389383|  A|2019-04-18 14:15:00|229.9428571| 27.62430996| 7.202027802|\n",
      "|  1|354010389388|  A|2019-04-19 19:00:00|      227.3|-8.171044489|-0.835037093|\n",
      "|  1|354010389388|  A|2019-04-20 03:15:00|      230.2| 3.993486188| 3.034862245|\n",
      "|  1|354010389385|  A|2019-04-20 03:30:00|     230.24| 50.29094031|-1.509997168|\n",
      "|  1|354010389381|  A|2019-04-18 11:30:00|227.8142857| 9.528651138| 14.83348842|\n",
      "|  1|354010389381|  A|2019-04-19 23:30:00|229.8285714|-19.90165392| 8.314101226|\n",
      "|  1|354019022842|  A|2019-04-20 17:15:00|      228.2|         0.0|         0.0|\n",
      "|  1|354010389385|  A|2019-04-18 00:15:00|     231.24| 27.93176645| 2.733648908|\n",
      "|  1|354010389381|  A|2019-04-18 01:00:00|      233.2|-22.42634144|-7.410992253|\n",
      "+---+------------+---+-------------------+-----------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T07:32:57.332885Z",
     "start_time": "2019-07-14T07:32:55.999390Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = input_lh_tp_node_ui.select(\"node\").distinct().count()\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T07:33:24.803070Z",
     "start_time": "2019-07-14T07:33:24.657130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[node: double, phase: double, rsquare: double, r: double, x: double, b0: double, b1: double]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(np.random.random((1, 7)))\n",
    "secondary_df = spark.createDataFrame(df, schema=['node', 'phase', 'rsquare', 'r', 'x', 'b0', 'b1'])\n",
    "primary_list = []\n",
    "secondary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T07:33:31.194870Z",
     "start_time": "2019-07-14T07:33:31.176627Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.24349</td>\n",
       "      <td>0.017483</td>\n",
       "      <td>0.606333</td>\n",
       "      <td>0.455034</td>\n",
       "      <td>0.494933</td>\n",
       "      <td>0.152829</td>\n",
       "      <td>0.272122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6\n",
       "0  0.24349  0.017483  0.606333  0.455034  0.494933  0.152829  0.272122"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T07:33:33.819353Z",
     "start_time": "2019-07-14T07:33:32.847244Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|               node|              phase|           rsquare|                 r|                 x|                b0|                b1|\n",
      "+-------------------+-------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|0.24348994365560517|0.01748258344313236|0.6063326807384616|0.4550337208839601|0.4949326310635016|0.1528285991805417|0.2721223319367121|\n",
      "+-------------------+-------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "secondary_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T07:33:36.405830Z",
     "start_time": "2019-07-14T07:33:36.190384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+---+-------------------+-----------+------------+------------+\n",
      "| LV|        NODE| L1|          DATA_TIME|          U|          IR|          IX|\n",
      "+---+------------+---+-------------------+-----------+------------+------------+\n",
      "|  1|354010389381|  A|2019-04-18 08:00:00|229.1714286| 7.873935201| 20.07064388|\n",
      "|  1|354010389388|  A|2019-04-20 20:00:00|    225.875| 8.138034736| 6.975577399|\n",
      "|  1|354010389381|  A|2019-04-18 08:30:00|229.9428571| -0.98901879|-9.586840019|\n",
      "|  1|354010389386|  A|2019-04-19 12:30:00|     228.24|-0.100734942| 0.968079481|\n",
      "|  1|354019022841|  A|2019-04-20 13:15:00|      228.0| 1.269984777| 4.580591726|\n",
      "|  1|354013332709|  A|2019-04-18 01:30:00|231.7666667|  12.4127701|-2.413129017|\n",
      "|  1|354010389386|  A|2019-04-18 13:45:00|     229.26|-7.112555611| 12.62412582|\n",
      "|  1|354010389388|  A|2019-04-20 09:45:00|     227.55| 16.40039602| 1.763531565|\n",
      "|  1|354010389383|  A|2019-04-20 13:45:00|228.5428571| 19.45274478|-8.603976663|\n",
      "|  1|354013332709|  A|2019-04-20 14:00:00|229.1166667| 3.353540984| 7.726937515|\n",
      "|  1|354019022839|  A|2019-04-20 18:15:00|228.0857143| 3.748100646|-4.459997783|\n",
      "|  1|354010389383|  A|2019-04-18 14:15:00|229.9428571| 27.62430996| 7.202027802|\n",
      "|  1|354010389388|  A|2019-04-19 19:00:00|      227.3|-8.171044489|-0.835037093|\n",
      "|  1|354010389388|  A|2019-04-20 03:15:00|      230.2| 3.993486188| 3.034862245|\n",
      "|  1|354010389385|  A|2019-04-20 03:30:00|     230.24| 50.29094031|-1.509997168|\n",
      "|  1|354010389381|  A|2019-04-18 11:30:00|227.8142857| 9.528651138| 14.83348842|\n",
      "|  1|354010389381|  A|2019-04-19 23:30:00|229.8285714|-19.90165392| 8.314101226|\n",
      "|  1|354019022842|  A|2019-04-20 17:15:00|      228.2|         0.0|         0.0|\n",
      "|  1|354010389385|  A|2019-04-18 00:15:00|     231.24| 27.93176645| 2.733648908|\n",
      "|  1|354010389381|  A|2019-04-18 01:00:00|      233.2|-22.42634144|-7.410992253|\n",
      "+---+------------+---+-------------------+-----------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_lh_tp_node_ui.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T07:33:37.932027Z",
     "start_time": "2019-07-14T07:33:37.918344Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_corr_max_two_node_name(input_lh_tp_node_ui):\n",
    "    '''\n",
    "    得到电压相关性最大的两个节点的名称\n",
    "    :param data: 原始数据\n",
    "    :return: 节点名 （电表名称）\n",
    "    '''\n",
    "    # 生成两两节点组合的名称集合 node_couple\n",
    "    node_array = np.array(input_lh_tp_node_ui.select(\"node\").distinct().collect()).tolist()\n",
    "    node_couple = []\n",
    "    for i in range(len(node_array)):\n",
    "        for j in range(i + 1, len(node_array)):\n",
    "            for p in ['A', 'B', 'C']:\n",
    "                if i != j:\n",
    "                    node_couple.append([node_array[i][0], node_array[j][0], p])\n",
    "    print('node_num: %s' %len(node_couple))\n",
    "    # 利用电压相关系数找出相关性最大的两个点\n",
    "    u_r2 = []\n",
    "    for no1, no2, phase in node_couple:\n",
    "        s1 = \"node == %s\" % no1\n",
    "        node1 = input_lh_tp_node_ui.filter(s1)\n",
    "        node1 = node1.select('data_time', 'u', 'l1').withColumnRenamed('u', 'u1')\n",
    "        s2 = \"node == %s\" % no2\n",
    "        node2 = input_lh_tp_node_ui.filter(s2)\n",
    "        node2 = node2.select('data_time', 'u', 'l1').withColumnRenamed('u', 'u2').withColumnRenamed('l1', 'l2')\n",
    "        node_join = node1.join(node2, (node1.data_time == node2.data_time) & (node1.l1 == node2.l2))\n",
    "        node_join = node_join.coalesce(10)\n",
    "        u_rsquare = node_join.corr('u1', 'u2')\n",
    "        u_r2.append([no1, no2, phase, u_rsquare])\n",
    "    name_u_r2 = [\"no1\", \"no2\", \"phase\", \"u_rsquare\"]\n",
    "    u_r2 = pd.DataFrame(columns=name_u_r2, data=u_r2)\n",
    "    node_x, node_y, phase_v, u_rsquare_best = u_r2[u_r2['u_rsquare'] == u_r2['u_rsquare'].max()].iloc[0,]\n",
    "    print(\"In this circulation, node {} and node {} is best!\".format(node_x, node_y))\n",
    "    return node_x, node_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T07:36:25.045000Z",
     "start_time": "2019-07-14T07:33:40.151588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 correlation data num of partition : 1\n",
      "node_num: 198\n",
      "In this circulation, node 354010389384 and node 354019022841 is best!\n"
     ]
    }
   ],
   "source": [
    "q = 0\n",
    "print(\"{} correlation data num of partition : {}\".format(q, input_lh_tp_node_ui.rdd.getNumPartitions()))\n",
    "node_x, node_y = get_corr_max_two_node_name(input_lh_tp_node_ui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T07:41:08.908507Z",
     "start_time": "2019-07-14T07:41:08.880574Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "def linear_regression(node_x, node_y, node_join_xy, phase='A'):\n",
    "    '''\n",
    "    两节点在单个相位上做线性回归分析\n",
    "    :param node_x: node_x name\n",
    "    :param node_y: node_y name\n",
    "    :node_join_xy: node_x node_y拼接后的数据\n",
    "    :param phase: 相位\n",
    "    :return: 回归系数列表\n",
    "    '''\n",
    "    # 筛选出单相位的数据\n",
    "    s3 = 'l1 == \"%s\"' % phase\n",
    "    node_join_p = node_join_xy.filter(s3)\n",
    "    node_join_p = node_join_p.drop('node1', 'node2', 'l1')\n",
    "    assembler = VectorAssembler(inputCols=[\"u1\", \"ir1\", \"ix1\", \"ir2\", \"ix2\"], outputCol=\"features\")\n",
    "    output = assembler.transform(node_join_p)\n",
    "    label_features = output.select(\"features\", \"u2\").toDF('features', 'label')\n",
    "    lr = LinearRegression(maxIter=5, elasticNetParam=0.8)\n",
    "    lrModel = lr.fit(label_features)\n",
    "    trainingSummary = lrModel.summary\n",
    "    param = [node_x, node_y, phase, trainingSummary.r2,\n",
    "             lrModel.intercept,\n",
    "             lrModel.coefficients[0],\n",
    "             lrModel.coefficients[1],\n",
    "             lrModel.coefficients[2],\n",
    "             lrModel.coefficients[3],\n",
    "             lrModel.coefficients[4]]\n",
    "    return param\n",
    "\n",
    "# from pyspark.ml.regression import LinearRegression\n",
    "def get_linear_regression_param_list(data, node_x, node_y):\n",
    "    '''\n",
    "    两节点在A、B、C三个相位上分别做线性回归\n",
    "    :param data: 原始数据\n",
    "    :param node_x: node_x name\n",
    "    :param node_y: node_y name\n",
    "    :return: 不同相位的回归系数 ~ spark-df/ node_x,node_y合并后的数据\n",
    "    '''\n",
    "    # 生成做回归分析的数据\n",
    "    s_x = \"node == {}\".format(node_x)\n",
    "    s_y = \"node == {}\".format(node_y)\n",
    "    nodex = data.filter(s_x)\n",
    "    nodey = data.filter(s_y)\n",
    "    nodex = nodex.withColumnRenamed('node', 'node1').withColumnRenamed('u', 'u1').withColumnRenamed('ir',\n",
    "                                                                                                    'ir1').withColumnRenamed(\n",
    "        'ix', 'ix1')\n",
    "    nodey = nodey.withColumnRenamed('node', 'node2').withColumnRenamed('l1', 'l2').withColumnRenamed('u',\n",
    "                                                                                                     'u2').withColumnRenamed(\n",
    "        'ir', 'ir2').withColumnRenamed('ix', 'ix2').withColumnRenamed('data_time', 'data_time2')\n",
    "    node_join_xy = nodex.join(nodey, ((nodex['data_time'] == nodey.data_time2) & (nodex['l1'] == nodey.l2)))\n",
    "    node_join_xy = node_join_xy.select('node1', 'node2', 'data_time', 'l1', 'u1', 'ir1', 'ix1', 'u2', 'ir2', 'ix2')\n",
    "    node_join_xy = node_join_xy.withColumn(\"ir2\", node_join_xy[\"ir2\"] * (-1))\n",
    "    node_join_xy = node_join_xy.withColumn(\"ix2\", node_join_xy[\"ix2\"] * (-1))\n",
    "    node_join_xy = node_join_xy.coalesce(10)\n",
    "    # 获得两表~三相位~的回归系数列表\n",
    "    param_list = []\n",
    "    for phase in ['A', 'B', 'C']:\n",
    "        param_list.append(linear_regression(node_x, node_y, node_join_xy, phase=phase))\n",
    "    name = ['node1', 'node2', 'phase', 'rsquare', 'b0', 'b1', 'r1', 'x1', 'r2', 'x2']\n",
    "    param_df = pd.DataFrame(columns=name, data=param_list)\n",
    "    param_dfs = spark.createDataFrame(param_df)\n",
    "    return param_dfs, node_join_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T07:41:21.817508Z",
     "start_time": "2019-07-14T07:41:10.385888Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 regression data num of partition : 1\n"
     ]
    }
   ],
   "source": [
    "# 得到两个点线性回归后的回归系数 - dfs\n",
    "# 19/07/14 03:34:09 WARN Column: Constructing trivially true equals predicate, 'data_time#13 = data_time#13'. Perhaps you need to use aliases.\n",
    "print(\"{} regression data num of partition : {}\".format(q, input_lh_tp_node_ui.rdd.getNumPartitions()))\n",
    "param_dfs, node_join_xy = get_linear_regression_param_list(input_lh_tp_node_ui, node_x, node_y)\n",
    "node_join_xy = node_join_xy.coalesce(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T07:44:30.683299Z",
     "start_time": "2019-07-14T07:44:29.791070Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+-----+------------------+-------------------+------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|       node1|       node2|phase|           rsquare|                 b0|                b1|                  r1|                  x1|                  r2|                  x2|\n",
      "+------------+------------+-----+------------------+-------------------+------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|354010389384|354019022841|    A|0.9959714867818102|0.14585444688059981|0.9993639686110559|0.001373987375185...|-0.09034395738334752|0.003069090407932825|-0.00244922333282...|\n",
      "|354010389384|354019022841|    B|0.9969139353209018| 1.7413196537090092|0.9922098666653818|-0.20479796583395626| 0.18314146966817496|-4.75272799809889E-4|0.004735318913674037|\n",
      "|354010389384|354019022841|    C|0.9965693684849772| 0.5610706920182464|0.9973047817358628|    0.13395822549546|0.042889167674767296|0.004570731678620694|-0.00121731665995...|\n",
      "+------------+------------+-----+------------------+-------------------+------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n",
      "+------------+------------+-------------------+---+-----+------------+-----------+-----+------------+------------+\n",
      "|       node1|       node2|          data_time| l1|   u1|         ir1|        ix1|   u2|         ir2|         ix2|\n",
      "+------------+------------+-------------------+---+-----+------------+-----------+-----+------------+------------+\n",
      "|354010389384|354019022841|2019-04-20 00:15:00|  A|229.9| 0.047857546|0.372109961|229.8| 0.094484993|-5.496218485|\n",
      "|354010389384|354019022841|2019-04-20 02:00:00|  A|230.3| 0.038044923|0.383327347|230.4| 0.534857013|-5.263553885|\n",
      "|354010389384|354019022841|2019-04-18 06:30:00|  A|230.2| 0.034796391|0.370889261|230.3|-2.346886788|-4.609007389|\n",
      "|354010389384|354019022841|2019-04-19 12:45:00|  A|228.3| 0.047070775|0.371586029|228.3|-1.094515581| -4.42018154|\n",
      "|354010389384|354019022841|2019-04-19 21:00:00|  A|227.5| 0.050504138|0.379147877|227.6|-1.642025629|-5.424284044|\n",
      "|354010389384|354019022841|2019-04-19 00:45:00|  A|230.6| 0.038497268|0.383738166|230.6|  0.03819512|-5.484517603|\n",
      "|354010389384|354019022841|2019-04-18 04:15:00|  A|231.8| 0.019645991|0.378211523|231.8| 0.041215634|-4.849623149|\n",
      "|354010389384|354019022841|2019-04-18 05:45:00|  A|230.5| 0.030904993|0.366813455|230.4|-0.747535625|-5.009282618|\n",
      "|354010389384|354019022841|2019-04-20 08:00:00|  A|229.2| 0.036342712|0.371537977|229.1|-1.472464975|-4.374997661|\n",
      "|354010389384|354019022841|2019-04-18 02:45:00|  A|231.5| 0.036481154|0.380149716|231.5| 0.362403918|-5.112176253|\n",
      "|354010389384|354019022841|2019-04-18 18:30:00|  A|228.8| 0.049066475|0.379061088|228.9|-4.438573326|-6.791397661|\n",
      "|354010389384|354019022841|2019-04-20 12:15:00|  A|228.0| 0.003284672|0.373146271|227.9|-13.30590962|-12.92730601|\n",
      "|354010389384|354019022841|2019-04-19 18:30:00|  A|228.4| 0.048577937|0.378732072|228.5|-0.562595032|-5.164005001|\n",
      "|354010389384|354019022841|2019-04-18 23:00:00|  A|229.2| 0.051155089|0.378496329|229.2|-1.639703779|-5.335006092|\n",
      "|354010389384|354019022841|2019-04-20 22:30:00|  A|229.3|-0.764692012| 0.57761497|229.3|-1.541843151| -6.46264228|\n",
      "|354010389384|354019022841|2019-04-20 20:15:00|  A|226.4| 0.050978778| 0.37761677|226.4|-0.564557014|-5.518050291|\n",
      "|354010389384|354019022841|2019-04-20 07:15:00|  A|229.7| 0.032288894|0.362662103|229.7|-2.607668015|-3.978666905|\n",
      "|354010389384|354019022841|2019-04-19 10:45:00|  A|228.4| 0.031892705|0.368585893|228.4|-1.560964024|-4.326751942|\n",
      "|354010389384|354019022841|2019-04-18 18:15:00|  A|229.2| 0.051106573|0.382332041|229.0|  -1.1070713|-4.925105823|\n",
      "|354010389384|354019022841|2019-04-19 14:15:00|  A|229.7| 0.046342405|0.374180256|229.6|-3.104507553|-4.844885339|\n",
      "+------------+------------+-------------------+---+-----+------------+-----------+-----+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_dfs.show()\n",
    "node_join_xy.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T07:44:31.014581Z",
     "start_time": "2019-07-14T07:44:30.995136Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'updata_node_couple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-b3db6c2ed85d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 更新输入数据：从原始数据中删除两个子节点，添加新的父节点\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minput_lh_tp_node_ui\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdata_node_couple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_lh_tp_node_ui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_join_xy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_dfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"before modify,input_lh_tp_node_ui num of partition : {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_lh_tp_node_ui\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minput_lh_tp_node_ui\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_lh_tp_node_ui\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"after modify,input_lh_tp_node_ui's partition: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_lh_tp_node_ui\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'updata_node_couple' is not defined"
     ]
    }
   ],
   "source": [
    "# 更新输入数据：从原始数据中删除两个子节点，添加新的父节点\n",
    "input_lh_tp_node_ui = updata_node_couple(input_lh_tp_node_ui, node_join_xy, param_dfs, node_x, node_y, q)\n",
    "print(\"before modify,input_lh_tp_node_ui num of partition : {}\".format(input_lh_tp_node_ui.rdd.getNumPartitions()))\n",
    "input_lh_tp_node_ui = input_lh_tp_node_ui.coalesce(10)\n",
    "print(\"after modify,input_lh_tp_node_ui's partition: {}\".format(input_lh_tp_node_ui.rdd.getNumPartitions()))\n",
    "print('%s loop new_data lines_num:%s' % (q, input_lh_tp_node_ui.count()))\n",
    "print('other running time: %s Seconds' % (time.time() - a))\n",
    "\n",
    "# 主副表单条数据生成、添加\n",
    "s1_dfs, s2_dfs, p1_list, p2_list = get_primary_secondary_single_data(param_dfs, node_x, node_y, q)\n",
    "primary_list.append(p1_list)\n",
    "primary_list.append(p2_list)\n",
    "secondary_df = secondary_df.union(s1_dfs)\n",
    "secondary_df = secondary_df.union(s2_dfs)\n",
    "secondary_df = secondary_df.coalesce(10)\n",
    "print('%s all running time: %s' % (q, time.time() - a))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
