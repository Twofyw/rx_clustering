{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Spark and build Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import findspark\n",
    "findspark.init('/home/ywx-data/spark/spark-2.4.3-bin-hadoop2.7')\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(None).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and show raw data with Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = spark.read.csv(\"lh_tp_node_ui.csv\", header=True, inferSchema=True)\n",
    "input_lh_tp_node_ui = df_raw.filter('lv == 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = input_lh_tp_node_ui.select(\"node\").distinct().count()\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.random((1, 7)))\n",
    "secondary_df = spark.createDataFrame(df, schema=['node', 'phase', 'rsquare', 'r', 'x', 'b0', 'b1'])\n",
    "primary_list = []\n",
    "secondary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondary_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lh_tp_node_ui.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corr_max_two_node_name(input_lh_tp_node_ui):\n",
    "    '''\n",
    "    得到电压相关性最大的两个节点的名称\n",
    "    :param data: 原始数据\n",
    "    :return: 节点名 （电表名称）\n",
    "    '''\n",
    "    # 生成两两节点组合的名称集合 node_couple\n",
    "    node_array = np.array(input_lh_tp_node_ui.select(\"node\").distinct().collect()).tolist()\n",
    "    node_couple = []\n",
    "    for i in range(len(node_array)):\n",
    "        for j in range(i + 1, len(node_array)):\n",
    "            for p in ['A', 'B', 'C']:\n",
    "                if i != j:\n",
    "                    node_couple.append([node_array[i][0], node_array[j][0], p])\n",
    "    print('node_num: %s' %len(node_couple))\n",
    "    # 利用电压相关系数找出相关性最大的两个点\n",
    "    u_r2 = []\n",
    "    for no1, no2, phase in node_couple:\n",
    "        s1 = \"node == %s\" % no1\n",
    "        node1 = input_lh_tp_node_ui.filter(s1)\n",
    "        node1 = node1.select('data_time', 'u', 'l1').withColumnRenamed('u', 'u1')\n",
    "        s2 = \"node == %s\" % no2\n",
    "        node2 = input_lh_tp_node_ui.filter(s2)\n",
    "        node2 = node2.select('data_time', 'u', 'l1').withColumnRenamed('u', 'u2').withColumnRenamed('l1', 'l2')\n",
    "        node_join = node1.join(node2, (node1.data_time == node2.data_time) & (node1.l1 == node2.l2))\n",
    "        node_join = node_join.coalesce(10)\n",
    "        u_rsquare = node_join.corr('u1', 'u2')\n",
    "        u_r2.append([no1, no2, phase, u_rsquare])\n",
    "    name_u_r2 = [\"no1\", \"no2\", \"phase\", \"u_rsquare\"]\n",
    "    u_r2 = pd.DataFrame(columns=name_u_r2, data=u_r2)\n",
    "    node_x, node_y, phase_v, u_rsquare_best = u_r2[u_r2['u_rsquare'] == u_r2['u_rsquare'].max()].iloc[0,]\n",
    "    print(\"In this circulation, node {} and node {} is best!\".format(node_x, node_y))\n",
    "    return node_x, node_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 0\n",
    "print(\"{} correlation data num of partition : {}\".format(q, input_lh_tp_node_ui.rdd.getNumPartitions()))\n",
    "node_x, node_y = get_corr_max_two_node_name(input_lh_tp_node_ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cost matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lh_tp_node_ui.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lh_tp_node_ui.corr('U', 'IR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corr_matrix(input_lh_tp_node_ui):\n",
    "    # 假设节点列表可以存进master\n",
    "    distinct_nodes = [o[0] for o in input_lh_tp_node_ui.select(\"node\").distinct().collect()]\n",
    "    node_count = len(distinct_nodes)\n",
    "    pairRDD = input_lh_tp_node_ui.rdd.map(lambda r: (r['NODE'], (r['DATA_TIME'], r['L1'], r['U']))).groupByKey().map(lambda x: (x[0], x[1].data))\n",
    "    header = ['DATA_TIME', 'L1', 'U']\n",
    "    \n",
    "    def rdd_struct_corr(structs):\n",
    "        node1, node2 = (o[0] for o in structs)\n",
    "        df1, df2 = (pd.DataFrame(o[1], columns=header) for o in structs)\n",
    "        df_join = df1.merge(df2, on=('DATA_TIME', 'L1'), suffixes=('_l', '_r'))\n",
    "        # TODO: coalesce?\n",
    "        corr = df_join[['U_l', 'U_r']].corr().iloc[0,1]\n",
    "        return node1, node2, float(corr)\n",
    "\n",
    "    cartesian = pairRDD.cartesian(pairRDD)\n",
    "    corrs_rdd = cartesian.map(rdd_struct_corr).sortBy(lambda r: (r[0], r[1]))\n",
    "    corrs = np.array([o for o in corrs_rdd.map(lambda r: r[2]).collect()]).reshape((node_count, node_count))\n",
    "    return corrs\n",
    "    \n",
    "corrs = get_corr_matrix(input_lh_tp_node_ui)\n",
    "print(corrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 0\n",
    "print(\"{} correlation data num of partition : {}\".format(q, input_lh_tp_node_ui.rdd.getNumPartitions()))\n",
    "node_x, node_y = get_corr_max_two_node_name(input_lh_tp_node_ui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "def linear_regression(node_x, node_y, node_join_xy, phase='A'):\n",
    "    '''\n",
    "    两节点在单个相位上做线性回归分析\n",
    "    :param node_x: node_x name\n",
    "    :param node_y: node_y name\n",
    "    :node_join_xy: node_x node_y拼接后的数据\n",
    "    :param phase: 相位\n",
    "    :return: 回归系数列表\n",
    "    '''\n",
    "    # 筛选出单相位的数据\n",
    "    s3 = 'l1 == \"%s\"' % phase\n",
    "    node_join_p = node_join_xy.filter(s3)\n",
    "    node_join_p = node_join_p.drop('node1', 'node2', 'l1')\n",
    "    assembler = VectorAssembler(inputCols=[\"u1\", \"ir1\", \"ix1\", \"ir2\", \"ix2\"], outputCol=\"features\")\n",
    "    output = assembler.transform(node_join_p)\n",
    "    label_features = output.select(\"features\", \"u2\").toDF('features', 'label')\n",
    "    lr = LinearRegression(maxIter=5, elasticNetParam=0.8)\n",
    "    lrModel = lr.fit(label_features)\n",
    "    trainingSummary = lrModel.summary\n",
    "    param = [node_x, node_y, phase, trainingSummary.r2,\n",
    "             lrModel.intercept,\n",
    "             lrModel.coefficients[0],\n",
    "             lrModel.coefficients[1],\n",
    "             lrModel.coefficients[2],\n",
    "             lrModel.coefficients[3],\n",
    "             lrModel.coefficients[4]]\n",
    "    return param\n",
    "\n",
    "# from pyspark.ml.regression import LinearRegression\n",
    "def get_linear_regression_param_list(data, node_x, node_y):\n",
    "    '''\n",
    "    两节点在A、B、C三个相位上分别做线性回归\n",
    "    :param data: 原始数据\n",
    "    :param node_x: node_x name\n",
    "    :param node_y: node_y name\n",
    "    :return: 不同相位的回归系数 ~ spark-df/ node_x,node_y合并后的数据\n",
    "    '''\n",
    "    # 生成做回归分析的数据\n",
    "    s_x = \"node == {}\".format(node_x)\n",
    "    s_y = \"node == {}\".format(node_y)\n",
    "    nodex = data.filter(s_x)\n",
    "    nodey = data.filter(s_y)\n",
    "    nodex = nodex.withColumnRenamed('node', 'node1').withColumnRenamed('u', 'u1').withColumnRenamed('ir',\n",
    "                                                                                                    'ir1').withColumnRenamed(\n",
    "        'ix', 'ix1')\n",
    "    nodey = nodey.withColumnRenamed('node', 'node2').withColumnRenamed('l1', 'l2').withColumnRenamed('u',\n",
    "                                                                                                     'u2').withColumnRenamed(\n",
    "        'ir', 'ir2').withColumnRenamed('ix', 'ix2').withColumnRenamed('data_time', 'data_time2')\n",
    "    node_join_xy = nodex.join(nodey, ((nodex['data_time'] == nodey.data_time2) & (nodex['l1'] == nodey.l2)))\n",
    "    node_join_xy = node_join_xy.select('node1', 'node2', 'data_time', 'l1', 'u1', 'ir1', 'ix1', 'u2', 'ir2', 'ix2')\n",
    "    node_join_xy = node_join_xy.withColumn(\"ir2\", node_join_xy[\"ir2\"] * (-1))\n",
    "    node_join_xy = node_join_xy.withColumn(\"ix2\", node_join_xy[\"ix2\"] * (-1))\n",
    "    node_join_xy = node_join_xy.coalesce(10)\n",
    "    # 获得两表~三相位~的回归系数列表\n",
    "    param_list = []\n",
    "    for phase in ['A', 'B', 'C']:\n",
    "        param_list.append(linear_regression(node_x, node_y, node_join_xy, phase=phase))\n",
    "    name = ['node1', 'node2', 'phase', 'rsquare', 'b0', 'b1', 'r1', 'x1', 'r2', 'x2']\n",
    "    param_df = pd.DataFrame(columns=name, data=param_list)\n",
    "    param_dfs = spark.createDataFrame(param_df)\n",
    "    return param_dfs, node_join_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到两个点线性回归后的回归系数 - dfs\n",
    "# 19/07/14 03:34:09 WARN Column: Constructing trivially true equals predicate, 'data_time#13 = data_time#13'. Perhaps you need to use aliases.\n",
    "print(\"{} regression data num of partition : {}\".format(q, input_lh_tp_node_ui.rdd.getNumPartitions()))\n",
    "param_dfs, node_join_xy = get_linear_regression_param_list(input_lh_tp_node_ui, node_x, node_y)\n",
    "node_join_xy = node_join_xy.coalesce(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dfs.show()\n",
    "node_join_xy.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新输入数据：从原始数据中删除两个子节点，添加新的父节点\n",
    "input_lh_tp_node_ui = updata_node_couple(input_lh_tp_node_ui, node_join_xy, param_dfs, node_x, node_y, q)\n",
    "print(\"before modify,input_lh_tp_node_ui num of partition : {}\".format(input_lh_tp_node_ui.rdd.getNumPartitions()))\n",
    "input_lh_tp_node_ui = input_lh_tp_node_ui.coalesce(10)\n",
    "print(\"after modify,input_lh_tp_node_ui's partition: {}\".format(input_lh_tp_node_ui.rdd.getNumPartitions()))\n",
    "print('%s loop new_data lines_num:%s' % (q, input_lh_tp_node_ui.count()))\n",
    "print('other running time: %s Seconds' % (time.time() - a))\n",
    "\n",
    "# 主副表单条数据生成、添加\n",
    "s1_dfs, s2_dfs, p1_list, p2_list = get_primary_secondary_single_data(param_dfs, node_x, node_y, q)\n",
    "primary_list.append(p1_list)\n",
    "primary_list.append(p2_list)\n",
    "secondary_df = secondary_df.union(s1_dfs)\n",
    "secondary_df = secondary_df.union(s2_dfs)\n",
    "secondary_df = secondary_df.coalesce(10)\n",
    "print('%s all running time: %s' % (q, time.time() - a))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
